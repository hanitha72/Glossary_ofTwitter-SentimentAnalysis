{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4"
   },
   "source": [
    "<h1 align = 'center'> Mechine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linefinalar algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e"
   },
   "source": [
    "Only keeping the necessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sentiment.csv', encoding= 'latin1')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13871, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c0ec63b-cdf8-8e29-812b-0fbbfcea2929"
   },
   "source": [
    "Next, I am dropping the 'Neutral' sentiments as my goal was to only differentiate positive and negative tweets. After that, I am filtering the tweets so only valid texts and words remain.  Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...   Neutral\n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral\n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076"
   },
   "outputs": [],
   "source": [
    "#data = data[data.sentiment != \"Neutral\"]\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  2236\n",
      "Negative:  8493\n",
      "Neutral:  3142\n"
     ]
    }
   ],
   "source": [
    "print('Positive: ', len(data[ data['sentiment'] == 'Positive']))\n",
    "print('Negative: ', len(data[ data['sentiment'] == 'Negative']))\n",
    "print('Neutral: ', len(data[ data['sentiment'] == 'Neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(nb_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X,maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13871, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    8493\n",
       "Neutral     3142\n",
       "Positive    2236\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data.sentiment.apply(lambda x: 1 if x=='Positive' else (0 if x == 'Negative' else 2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8493\n",
       "2    3142\n",
       "1    2236\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9293, 100) (9293,)\n",
      "(4578, 100) (4578,)\n"
     ]
    }
   ],
   "source": [
    "Y = data.sentiment\n",
    " \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:maroon'>Model Building</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>Multi Nominal Naivebayes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "MNB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix on train data\n",
      " [[2257  968 2459]\n",
      " [ 345  369  783]\n",
      " [ 440  305 1367]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on train data\n",
    "MNB_trainpreds =MNB.predict(X_train)\n",
    "print(\"confusion_matrix on train data\\n\",confusion_matrix(Y_train,MNB_trainpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.40      0.52      5684\n",
      "           1       0.22      0.25      0.24      1497\n",
      "           2       0.30      0.65      0.41      2112\n",
      "\n",
      "    accuracy                           0.43      9293\n",
      "   macro avg       0.42      0.43      0.39      9293\n",
      "weighted avg       0.56      0.43      0.45      9293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for Train data:\n",
    "print(classification_report(y_true=Y_train,y_pred=MNB_trainpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix on train data\n",
      " [[1121  495 1193]\n",
      " [ 189  170  380]\n",
      " [ 224  151  655]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on validation data\n",
    "MNB_valpreds =MNB.predict(X_test)\n",
    "print(\"confusion_matrix on train data\\n\",confusion_matrix(Y_test,MNB_valpreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, ..., 1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_valpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.40      0.52      2809\n",
      "           1       0.21      0.23      0.22       739\n",
      "           2       0.29      0.64      0.40      1030\n",
      "\n",
      "    accuracy                           0.43      4578\n",
      "   macro avg       0.41      0.42      0.38      4578\n",
      "weighted avg       0.55      0.43      0.44      4578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for validation data:\n",
    "print(classification_report(y_true=Y_test,y_pred=MNB_valpreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evalution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4467280151342626\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# F1 Score for Train data:\n",
    "print(f1_score(Y_train, MNB_trainpreds, average= 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44251427000943966\n"
     ]
    }
   ],
   "source": [
    "# F1 Score for Validation data:\n",
    "print(f1_score(Y_test, MNB_valpreds, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>SGD Classifier</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_classifier = SGDClassifier(loss='hinge',\n",
    "                                         penalty='l2',  alpha=0.0001, \n",
    "                                         l1_ratio=0.15,    fit_intercept=True, \n",
    "                                         max_iter=1000,    tol=0.001, shuffle=True, \n",
    "                                         verbose=0,        epsilon=0.1, n_jobs=None, \n",
    "                                         random_state=None, learning_rate='optimal', \n",
    "                                         eta0=0.0,          power_t=0.5, early_stopping=False, \n",
    "                                         validation_fraction=0.1, n_iter_no_change=5, \n",
    "                                         class_weight=None, warm_start=False, average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit on train data\n",
    "sgd_classifier.fit(X=X_train, y= Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on train data\n",
    "sgd_trainPreds = sgd_classifier.predict(X_train)\n",
    "\n",
    "## Predict on validation data\n",
    "sgd_valPreds = sgd_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DATA ACCURACY 0.5136123964274185\n",
      "\n",
      "Train data f1-score 0.4853558526700875\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAIN DATA ACCURACY\",accuracy_score(Y_train, sgd_trainPreds))\n",
    "print(\"\\nTrain data f1-score\", f1_score(Y_train, sgd_trainPreds,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DATA ACCURACY 0.5192223678462211\n",
      "\n",
      "Validation data f1-score 0.4877272185488563\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidation DATA ACCURACY\",accuracy_score(Y_test,sgd_valPreds))\n",
    "print(\"\\nValidation data f1-score\", f1_score(Y_test, sgd_valPreds,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style='color:blue'>SVM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit on train data\n",
    "SVC.fit(X=X_train, y= Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on train data\n",
    "svm_trainPreds = SVC.predict(X_train)\n",
    "\n",
    "## Predict on validation data\n",
    "svm_valPreds = SVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Conf Matrix : \n",
      " [[5678    0    6]\n",
      " [1444   49    4]\n",
      " [2048    6   58]]\n",
      "\n",
      "TRAIN DATA ACCURACY 0.6225115678467664\n",
      "\n",
      "Train data f1-score 0.4898710302596147\n"
     ]
    }
   ],
   "source": [
    "### Train data accuracy\n",
    "\n",
    "\n",
    "print(\"TRAIN Conf Matrix : \\n\", confusion_matrix(Y_train, svm_trainPreds))\n",
    "print(\"\\nTRAIN DATA ACCURACY\",accuracy_score(Y_train, svm_trainPreds))\n",
    "print(\"\\nTrain data f1-score\", f1_score(Y_train, svm_trainPreds,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Conf Matrix : \n",
      " [[2804    1    4]\n",
      " [ 725   11    3]\n",
      " [1011    5   14]]\n",
      "\n",
      "Validation DATA ACCURACY 0.6179554390563565\n"
     ]
    }
   ],
   "source": [
    "### Test data accuracy\n",
    "print(\"Validation Conf Matrix : \\n\", confusion_matrix(Y_test, svm_valPreds))\n",
    "print(\"\\nValidation DATA ACCURACY\",accuracy_score(Y_test,svm_valPreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>RandomForest</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "Classifier_RF = RandomForestClassifier(criterion='entropy',n_estimators=10,max_depth=100,bootstrap = True,max_features = 'sqrt'\n",
    "                                       ,min_samples_leaf=2,random_state=123,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=100, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=2, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=123,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on training data\n",
    "Classifier_RF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix on train data\n",
      " [[5515   76   93]\n",
      " [ 396 1036   65]\n",
      " [ 576   77 1459]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on train data\n",
    "RF_trainPreds = Classifier_RF.predict(X_train)\n",
    "print(\"confusion_matrix on train data\\n\",confusion_matrix(Y_train,RF_trainPreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      5684\n",
      "           1       0.87      0.69      0.77      1497\n",
      "           2       0.90      0.69      0.78      2112\n",
      "\n",
      "    accuracy                           0.86      9293\n",
      "   macro avg       0.87      0.78      0.82      9293\n",
      "weighted avg       0.87      0.86      0.86      9293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for Train data:\n",
    "print(classification_report(y_true=Y_train, y_pred=RF_trainPreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8200584281194284\n"
     ]
    }
   ],
   "source": [
    "# F1 Score for Train data:\n",
    "print(f1_score(Y_train, RF_trainPreds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix on train data\n",
      " [[2522   76  211]\n",
      " [ 508  163   68]\n",
      " [ 754   58  218]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on Validation data\n",
    "RF_valPreds = Classifier_RF.predict(X_test)\n",
    "print(\"confusion_matrix on train data\\n\",confusion_matrix(Y_test,RF_valPreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77      2809\n",
      "           1       0.55      0.22      0.31       739\n",
      "           2       0.44      0.21      0.29      1030\n",
      "\n",
      "    accuracy                           0.63      4578\n",
      "   macro avg       0.55      0.44      0.46      4578\n",
      "weighted avg       0.60      0.63      0.58      4578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report for Train data:\n",
    "print(classification_report(y_true=Y_test, y_pred=RF_valPreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45508427904369664\n"
     ]
    }
   ],
   "source": [
    "# F1 Score for Train data:\n",
    "print(f1_score(Y_test, RF_valPreds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(RF_valPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv(\"preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.rename(columns = {0:\"Pred\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  21,  49,  49],\n",
       "       [  0,   0,   0, ..., 989, 403, 563],\n",
       "       [  0,   0,   0, ..., 122,   1,   3],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 178, 875,   1],\n",
       "       [  0,   0,   0, ..., 282, 789,   1],\n",
       "       [  0,   0,   0, ..., 182, 767,  17]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.Pred.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:blue'>LSTM</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \"\"\"\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 256)          512000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               355152    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 867,743\n",
      "Trainable params: 867,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9293, 100) (9293, 3)\n",
      "(4578, 100) (4578, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  811,  161,  540],\n",
       "       [   0,    0,    0, ...,   91,  539,    1],\n",
       "       [   0,    0,    0, ..., 1992,   72, 1651],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  976,   14,    1],\n",
       "       [   0,    0,    0, ...,   37, 1478, 1424],\n",
       "       [   0,    0,    0, ...,  223,   12,    1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/7\n",
      " - 23s - loss: 0.8389 - accuracy: 0.6427\n",
      "Epoch 2/7\n",
      " - 24s - loss: 0.6844 - accuracy: 0.7050\n",
      "Epoch 3/7\n",
      " - 25s - loss: 0.6165 - accuracy: 0.7416\n",
      "Epoch 4/7\n",
      " - 24s - loss: 0.5685 - accuracy: 0.7633\n",
      "Epoch 5/7\n",
      " - 24s - loss: 0.5256 - accuracy: 0.7825\n",
      "Epoch 6/7\n",
      " - 24s - loss: 0.4885 - accuracy: 0.7994\n",
      "Epoch 7/7\n",
      " - 24s - loss: 0.4510 - accuracy: 0.8136\n",
      "Wall time: 2min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19f7b5c6788>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model.fit(X_train, Y_train, nb_epoch = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.94\n",
      "acc: 0.65\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 100)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct, nut_cnt, nut_correct = 0, 0, 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    \n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        elif np.argmax(Y_validate[x]) == 2:\n",
    "            nut_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "       \n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    elif np.argmax(Y_validate[x]) == 2:\n",
    "        nut_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_acc 53.23 %\n",
      "neg_acc 73.43 %\n",
      "Nuetral_acc 40.0 %\n"
     ]
    }
   ],
   "source": [
    "print('pos_acc', round(pos_correct/pos_cnt*100, 2), '%')\n",
    "print ('neg_acc', round(neg_correct/neg_cnt*100,2), \"%\")\n",
    "print('Nuetral_acc', round(nut_correct/nut_cnt*100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:blue'>NaiveBayes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nb = pd.read_csv('Sentiment.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_nb[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...   Neutral\n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral\n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test set\n",
    "train, test = train_test_split(data,test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n",
    "    words_cleaned = [word for word in words_filtered\n",
    "        if 'http' not in word\n",
    "        and not word.startswith('@')\n",
    "        and not word.startswith('#')\n",
    "        and word != 'RT']\n",
    "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n",
    "    tweets.append((words_cleaned,row.sentiment))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['note',\n",
       "   'rand',\n",
       "   'paul:',\n",
       "   \"don't\",\n",
       "   'pick',\n",
       "   'fights',\n",
       "   'with',\n",
       "   'chris',\n",
       "   'christie.'],\n",
       "  'Neutral'),\n",
       " (['know',\n",
       "   \"he's\",\n",
       "   'sorta',\n",
       "   'busy,',\n",
       "   'but',\n",
       "   'post-debate',\n",
       "   'discussion',\n",
       "   'just',\n",
       "   \"isn't\",\n",
       "   'the',\n",
       "   'same',\n",
       "   'without'],\n",
       "  'Negative')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "tweets = []\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n",
    "    words_cleaned = [word for word in words_filtered\n",
    "        if 'http' not in word\n",
    "        and not word.startswith('@')\n",
    "        and not word.startswith('#')\n",
    "        and word != 'RT']\n",
    "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n",
    "    tweets.append((words_cleaned,row.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting word features\n",
    "def get_words_in_tweets(tweets):\n",
    "    all = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all.extend(words)\n",
    "    return all\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    features = wordlist.keys()\n",
    "    return features\n",
    "w_features = get_word_features(get_words_in_tweets(tweets))\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in w_features:\n",
    "        features['containts(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Naive Bayes classifier\n",
    "training_set = nltk.classify.apply_features(extract_features,tweets)\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = test[ test['sentiment'] == 'Positive']\n",
    "test_pos = test_pos['text']\n",
    "test_neg = test[ test['sentiment'] == 'Negative']\n",
    "test_neg = test_neg['text']\n",
    "test_nut = test[ test['sentiment'] == 'Neutral']\n",
    "test_nut = test_nut['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Negative]: 744/893 \n",
      "[Neutral]: 98/304 \n",
      "[Positive]: 65/191 \n"
     ]
    }
   ],
   "source": [
    "neg_cnt = 0\n",
    "pos_cnt = 0\n",
    "nut_cnt = 0\n",
    "\n",
    "for obj in test_neg: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Negative'): \n",
    "        neg_cnt = neg_cnt + 1\n",
    "        \n",
    "for obj in test_pos: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Positive'): \n",
    "        pos_cnt = pos_cnt + 1\n",
    "\n",
    "for obj in test_nut: \n",
    "    res =  classifier.classify(extract_features(obj.split()))\n",
    "    if(res == 'Neutral'): \n",
    "        nut_cnt = nut_cnt + 1\n",
    "        \n",
    "print('[Negative]: %s/%s '  % (neg_cnt,len(test_neg))) \n",
    "print('[Neutral]: %s/%s '  % (nut_cnt, len(test_nut)))\n",
    "print('[Positive]: %s/%s '  % (pos_cnt, len(test_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Percentage: \n",
      "[Negative]:  83.31\n",
      "[Neutral]:  32.24\n",
      "[Positive]: 34.03\n"
     ]
    }
   ],
   "source": [
    "print(\"In Percentage: \")\n",
    "\n",
    "print('[Negative]: ', round(neg_cnt/len(test_neg)*100,2))\n",
    "print('[Neutral]: ', round(nut_cnt/len(test_nut)*100,2))\n",
    "print('[Positive]:', round(pos_cnt/len(test_pos)*100,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13871, 100)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on Unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bramin_tweetDF = pd.read_csv('#BrahmanismRobsOBCReservationhashtagTweets.csv', encoding='latin1')\n",
    "shame_tweetDF = pd.read_csv('#ShameOnNews18hashtagTweets.csv', encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1790, 12)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bramin_tweetDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "bramin_tweet = bramin_tweetDF[['text']]\n",
    "shame_tweet = shame_tweetDF[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1790, 1), (1800, 1))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bramin_tweet.shape, shame_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = bramin_tweetDF\n",
    "final2 = shame_tweetDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>acctdesc</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>usercreatedts</th>\n",
       "      <th>tweetcreatedts</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>TweetType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PunniyakotiMu</td>\n",
       "      <td>Tamilian by Language. Indian by Land. Humane b...</td>\n",
       "      <td>Chennai, India</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>3949</td>\n",
       "      <td>2010-12-15 04:01:28</td>\n",
       "      <td>2020-07-21 03:01:18</td>\n",
       "      <td>711</td>\n",
       "      <td>OBC population is 52%.\\nOBC Reservation is 27%...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contractornesa5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>425</td>\n",
       "      <td>132</td>\n",
       "      <td>2810</td>\n",
       "      <td>2019-05-31 03:23:24</td>\n",
       "      <td>2020-07-21 02:58:26</td>\n",
       "      <td>14</td>\n",
       "      <td>@RangarajPandeyR In 1990, when VP Singh implem...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vcrasu</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>5</td>\n",
       "      <td>1027</td>\n",
       "      <td>2020-06-04 08:45:04</td>\n",
       "      <td>2020-07-21 02:58:20</td>\n",
       "      <td>344</td>\n",
       "      <td>52% OBC's are Making two third of Hindus. Now ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username                                           acctdesc  \\\n",
       "0    PunniyakotiMu  Tamilian by Language. Indian by Land. Humane b...   \n",
       "1  Contractornesa5                                                NaN   \n",
       "2           vcrasu                                 \n",
       "\n",
       "         location  following  followers  totaltweets        usercreatedts  \\\n",
       "0  Chennai, India         67         14         3949  2010-12-15 04:01:28   \n",
       "1             NaN        425        132         2810  2019-05-31 03:23:24   \n",
       "2             NaN        158          5         1027  2020-06-04 08:45:04   \n",
       "\n",
       "        tweetcreatedts  retweetcount  \\\n",
       "0  2020-07-21 03:01:18           711   \n",
       "1  2020-07-21 02:58:26            14   \n",
       "2  2020-07-21 02:58:20           344   \n",
       "\n",
       "                                                text hashtags  TweetType  \n",
       "0  OBC population is 52%.\\nOBC Reservation is 27%...       []          1  \n",
       "1  @RangarajPandeyR In 1990, when VP Singh implem...       []          1  \n",
       "2  52% OBC's are Making two third of Hindus. Now ...       []          1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnseenData cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataclean(tweet):\n",
    "    tweet['text'] = tweet['text'].apply(lambda x: x.lower())\n",
    "    tweet['text'] = tweet['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    for idx,row in tweet.iterrows():\n",
    "        row[0] = row[0].replace('rt',' ')     \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "data1 = get_dataclean(bramin_tweet)\n",
    "data2 = get_dataclean(shame_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data shape\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obc population is 52\\nobc reservation is 27\\ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rangarajpandeyr in 1990 when vp singh implemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52 obcs are making two third of hindus now mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  obc population is 52\\nobc reservation is 27\\ne...\n",
       "1  rangarajpandeyr in 1990 when vp singh implemen...\n",
       "2  52 obcs are making two third of hindus now mod..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of Unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(nb_words=max_fatures, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function for tokenizing\n",
    "\n",
    "def Get_tokens(data):\n",
    "    tokenizer.fit_on_texts(data['text'].values)\n",
    "    g_token = tokenizer.texts_to_sequences(data['text'].values)\n",
    "    g_pad = pad_sequences(g_token, maxlen=100)\n",
    "    return g_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "token1 = Get_tokens(data1)\n",
    "token2 = Get_tokens(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1790, (1790, 100))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token1),  token1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Model(token,model):\n",
    "    prds = model.predict(token)\n",
    "    return prds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(sentiment_score):\n",
    "    senti = []\n",
    "    for i in sentiment_score:\n",
    "        if i == 1:\n",
    "            senti.append(\"Positive\")\n",
    "        elif i == 0:\n",
    "            senti.append(\"Negative\")\n",
    "        else:\n",
    "            senti.append(\"Nuetral\")\n",
    "    return senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = Get_Model(token1, SVC)\n",
    "df2 = Get_Model(token2, SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_type(df1)\n",
    "df2 = get_type(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_RFScore = Get_Model(token1, Classifier_RF)\n",
    "df2_RFScore = Get_Model(token2, Classifier_RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_RF = get_type(df1_RFScore)\n",
    "df2_RF = get_type(df2_RFScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Nuetral',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_RF[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_LSTM(token,model):\n",
    "    prds = model.predict(token)\n",
    "    score = [np.argmax(i) for i in prds]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_lstmScore = Get_LSTM(token1, model)\n",
    "df2_lstmScore = Get_LSTM(token2, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_lstm = get_type(df1_lstmScore)\n",
    "df2_lstm = get_type(df2_lstmScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_lstm[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NaiveBayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_NB_pred(data,model):\n",
    "    res = []\n",
    "    for obj in data.text:\n",
    "        res.append(model.classify(extract_features(obj.split())))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = Get_NB_pred(bramin_tweet, classifier)\n",
    "result2 = Get_NB_pred(shame_tweet, classifier)\n",
    "# result3 = Get_NB_pred(stepDown_tweet, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative',\n",
       " 'Negative',\n",
       " 'Neutral',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Neutral',\n",
       " 'Negative',\n",
       " 'Neutral',\n",
       " 'Negative',\n",
       " 'Negative']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obc population is 52\\nobc reservation is 27\\ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rangarajpandeyr in 1990 when vp singh implemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52 obcs are making two third of hindus now mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please suppo  amp maximum  \\n\\nbrahmanismrobso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of 89 secretaries in modi govt there are just ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  obc population is 52\\nobc reservation is 27\\ne...\n",
       "1  rangarajpandeyr in 1990 when vp singh implemen...\n",
       "2  52 obcs are making two third of hindus now mod...\n",
       "3  please suppo  amp maximum  \\n\\nbrahmanismrobso...\n",
       "4  of 89 secretaries in modi govt there are just ..."
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\hanit\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data1['polarity'] = data1['text'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "data2['polarity'] = data2['text'].map(lambda text: TextBlob(text).sentiment.polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function for converting sentiment type as Positive if sentiment score is (>0), neutral if sentiment score is (=0), \n",
    "#negative if sentiment score is (<0)\n",
    "\n",
    "def Get_Polarity(_polarity):\n",
    "    sentiment = \"Positive\"\n",
    "    if _polarity > 0:\n",
    "        sentiment= 'Positive'\n",
    "                \n",
    "    elif _polarity == 0:\n",
    "        sentiment = 'Neutral'\n",
    "            \n",
    "    else:\n",
    "        sentiment = 'Negative'\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity1 = data1.polarity.apply(Get_Polarity)\n",
    "polarity2 = data2.polarity.apply(Get_Polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    Negative\n",
       " 1     Neutral\n",
       " 2    Positive\n",
       " 3     Neutral\n",
       " 4     Neutral\n",
       " 5    Positive\n",
       " 6    Positive\n",
       " 7     Neutral\n",
       " 8    Negative\n",
       " 9    Positive\n",
       " Name: polarity, dtype: object,\n",
       " pandas.core.series.Series)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity1[:10], type(polarity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>acctdesc</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>usercreatedts</th>\n",
       "      <th>tweetcreatedts</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>TweetType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PunniyakotiMu</td>\n",
       "      <td>Tamilian by Language. Indian by Land. Humane b...</td>\n",
       "      <td>Chennai, India</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>3949</td>\n",
       "      <td>2010-12-15 04:01:28</td>\n",
       "      <td>2020-07-21 03:01:18</td>\n",
       "      <td>711</td>\n",
       "      <td>OBC population is 52%.\\nOBC Reservation is 27%...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contractornesa5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>425</td>\n",
       "      <td>132</td>\n",
       "      <td>2810</td>\n",
       "      <td>2019-05-31 03:23:24</td>\n",
       "      <td>2020-07-21 02:58:26</td>\n",
       "      <td>14</td>\n",
       "      <td>@RangarajPandeyR In 1990, when VP Singh implem...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vcrasu</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>5</td>\n",
       "      <td>1027</td>\n",
       "      <td>2020-06-04 08:45:04</td>\n",
       "      <td>2020-07-21 02:58:20</td>\n",
       "      <td>344</td>\n",
       "      <td>52% OBC's are Making two third of Hindus. Now ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ngdharan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91</td>\n",
       "      <td>94</td>\n",
       "      <td>5323</td>\n",
       "      <td>2012-03-29 07:34:49</td>\n",
       "      <td>2020-07-21 02:56:27</td>\n",
       "      <td>123</td>\n",
       "      <td>Please Support &amp;amp; Maximum RT...\\n\\n#Brahman...</td>\n",
       "      <td>[{'text': 'BrahmanismRobsOBCReservation', 'ind...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EzhilJerry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tamil Nadu, India</td>\n",
       "      <td>133</td>\n",
       "      <td>88</td>\n",
       "      <td>11929</td>\n",
       "      <td>2011-07-10 03:39:34</td>\n",
       "      <td>2020-07-21 02:55:27</td>\n",
       "      <td>3</td>\n",
       "      <td>Of 89 secretaries in Modi govt, there are just...</td>\n",
       "      <td>[{'text': 'BrahmanismRobsOBCReservation', 'ind...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>sabarivasandmk</td>\n",
       "      <td>Black reds are my life </td>\n",
       "      <td>Kanchipuram, Tamilnadu</td>\n",
       "      <td>206</td>\n",
       "      <td>68</td>\n",
       "      <td>1596</td>\n",
       "      <td>2020-05-05 07:50:45</td>\n",
       "      <td>2020-07-20 15:24:33</td>\n",
       "      <td>170</td>\n",
       "      <td>It took  65 years for my caste which comes int...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>sabarivasandmk</td>\n",
       "      <td>Black reds are my life </td>\n",
       "      <td>Kanchipuram, Tamilnadu</td>\n",
       "      <td>206</td>\n",
       "      <td>68</td>\n",
       "      <td>1596</td>\n",
       "      <td>2020-05-05 07:50:45</td>\n",
       "      <td>2020-07-20 15:24:31</td>\n",
       "      <td>124</td>\n",
       "      <td>Please Support &amp;amp; Maximum RT...\\n\\n#Brahman...</td>\n",
       "      <td>[{'text': 'BrahmanismRobsOBCReservation', 'ind...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>sabarivasandmk</td>\n",
       "      <td>Black reds are my life </td>\n",
       "      <td>Kanchipuram, Tamilnadu</td>\n",
       "      <td>206</td>\n",
       "      <td>68</td>\n",
       "      <td>1596</td>\n",
       "      <td>2020-05-05 07:50:45</td>\n",
       "      <td>2020-07-20 15:24:29</td>\n",
       "      <td>300</td>\n",
       "      <td>The Modi govt wants to include salary earnings...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>sabarivasandmk</td>\n",
       "      <td>Black reds are my life </td>\n",
       "      <td>Kanchipuram, Tamilnadu</td>\n",
       "      <td>206</td>\n",
       "      <td>68</td>\n",
       "      <td>1596</td>\n",
       "      <td>2020-05-05 07:50:45</td>\n",
       "      <td>2020-07-20 15:24:26</td>\n",
       "      <td>255</td>\n",
       "      <td>The data was obtained thru RTI by @asahcu. OBC...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>sabarivasandmk</td>\n",
       "      <td>Black reds are my life </td>\n",
       "      <td>Kanchipuram, Tamilnadu</td>\n",
       "      <td>206</td>\n",
       "      <td>68</td>\n",
       "      <td>1596</td>\n",
       "      <td>2020-05-05 07:50:45</td>\n",
       "      <td>2020-07-20 15:24:21</td>\n",
       "      <td>170</td>\n",
       "      <td>OBCs of the land unite against the brahmin heg...</td>\n",
       "      <td>[{'text': 'BrahmanismRobsOBCReservation', 'ind...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1790 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             username                                           acctdesc  \\\n",
       "0       PunniyakotiMu  Tamilian by Language. Indian by Land. Humane b...   \n",
       "1     Contractornesa5                                                NaN   \n",
       "2              vcrasu                                 \n",
       "3            ngdharan                                                NaN   \n",
       "4          EzhilJerry                                                NaN   \n",
       "...               ...                                                ...   \n",
       "1785   sabarivasandmk              Black reds are my life    \n",
       "1786   sabarivasandmk              Black reds are my life    \n",
       "1787   sabarivasandmk              Black reds are my life    \n",
       "1788   sabarivasandmk              Black reds are my life    \n",
       "1789   sabarivasandmk              Black reds are my life    \n",
       "\n",
       "                    location  following  followers  totaltweets  \\\n",
       "0             Chennai, India         67         14         3949   \n",
       "1                        NaN        425        132         2810   \n",
       "2                        NaN        158          5         1027   \n",
       "3                        NaN         91         94         5323   \n",
       "4          Tamil Nadu, India        133         88        11929   \n",
       "...                      ...        ...        ...          ...   \n",
       "1785  Kanchipuram, Tamilnadu        206         68         1596   \n",
       "1786  Kanchipuram, Tamilnadu        206         68         1596   \n",
       "1787  Kanchipuram, Tamilnadu        206         68         1596   \n",
       "1788  Kanchipuram, Tamilnadu        206         68         1596   \n",
       "1789  Kanchipuram, Tamilnadu        206         68         1596   \n",
       "\n",
       "            usercreatedts       tweetcreatedts  retweetcount  \\\n",
       "0     2010-12-15 04:01:28  2020-07-21 03:01:18           711   \n",
       "1     2019-05-31 03:23:24  2020-07-21 02:58:26            14   \n",
       "2     2020-06-04 08:45:04  2020-07-21 02:58:20           344   \n",
       "3     2012-03-29 07:34:49  2020-07-21 02:56:27           123   \n",
       "4     2011-07-10 03:39:34  2020-07-21 02:55:27             3   \n",
       "...                   ...                  ...           ...   \n",
       "1785  2020-05-05 07:50:45  2020-07-20 15:24:33           170   \n",
       "1786  2020-05-05 07:50:45  2020-07-20 15:24:31           124   \n",
       "1787  2020-05-05 07:50:45  2020-07-20 15:24:29           300   \n",
       "1788  2020-05-05 07:50:45  2020-07-20 15:24:26           255   \n",
       "1789  2020-05-05 07:50:45  2020-07-20 15:24:21           170   \n",
       "\n",
       "                                                   text  \\\n",
       "0     OBC population is 52%.\\nOBC Reservation is 27%...   \n",
       "1     @RangarajPandeyR In 1990, when VP Singh implem...   \n",
       "2     52% OBC's are Making two third of Hindus. Now ...   \n",
       "3     Please Support &amp; Maximum RT...\\n\\n#Brahman...   \n",
       "4     Of 89 secretaries in Modi govt, there are just...   \n",
       "...                                                 ...   \n",
       "1785  It took  65 years for my caste which comes int...   \n",
       "1786  Please Support &amp; Maximum RT...\\n\\n#Brahman...   \n",
       "1787  The Modi govt wants to include salary earnings...   \n",
       "1788  The data was obtained thru RTI by @asahcu. OBC...   \n",
       "1789  OBCs of the land unite against the brahmin heg...   \n",
       "\n",
       "                                               hashtags  TweetType  \n",
       "0                                                    []          1  \n",
       "1                                                    []          1  \n",
       "2                                                    []          1  \n",
       "3     [{'text': 'BrahmanismRobsOBCReservation', 'ind...          1  \n",
       "4     [{'text': 'BrahmanismRobsOBCReservation', 'ind...          1  \n",
       "...                                                 ...        ...  \n",
       "1785                                                 []          1  \n",
       "1786  [{'text': 'BrahmanismRobsOBCReservation', 'ind...          1  \n",
       "1787                                                 []          1  \n",
       "1788                                                 []          1  \n",
       "1789  [{'text': 'BrahmanismRobsOBCReservation', 'ind...          1  \n",
       "\n",
       "[1790 rows x 12 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_report(data, svmSer, rfSer, lstmSer, NBPreds, PolarityPreds):\n",
    "    report = pd.DataFrame()\n",
    "    report = data\n",
    "    report['SVM_Prediction'] = svmSer\n",
    "    report['RF_Prediction'] = rfSer\n",
    "    report['LSTM_Prediction'] = lstmSer\n",
    "    report['NaiveBayes_Prediction'] = NBPreds\n",
    "    report['Text_Blob_Prediction'] = PolarityPreds\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "report1 = Final_report(final1, df1, df1_RF,df1_lstm, result1, polarity1)\n",
    "report2 = Final_report(final2, df2, df2_RF, df2_lstm, result2, polarity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>acctdesc</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>usercreatedts</th>\n",
       "      <th>tweetcreatedts</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>TweetType</th>\n",
       "      <th>SVM_Prediction</th>\n",
       "      <th>RF_Prediction</th>\n",
       "      <th>LSTM_Prediction</th>\n",
       "      <th>NaiveBayes_Prediction</th>\n",
       "      <th>Text_Blob_Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PunniyakotiMu</td>\n",
       "      <td>Tamilian by Language. Indian by Land. Humane b...</td>\n",
       "      <td>Chennai, India</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>3949</td>\n",
       "      <td>2010-12-15 04:01:28</td>\n",
       "      <td>2020-07-21 03:01:18</td>\n",
       "      <td>711</td>\n",
       "      <td>OBC population is 52%.\\nOBC Reservation is 27%...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contractornesa5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>425</td>\n",
       "      <td>132</td>\n",
       "      <td>2810</td>\n",
       "      <td>2019-05-31 03:23:24</td>\n",
       "      <td>2020-07-21 02:58:26</td>\n",
       "      <td>14</td>\n",
       "      <td>@RangarajPandeyR In 1990, when VP Singh implem...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vcrasu</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>5</td>\n",
       "      <td>1027</td>\n",
       "      <td>2020-06-04 08:45:04</td>\n",
       "      <td>2020-07-21 02:58:20</td>\n",
       "      <td>344</td>\n",
       "      <td>52% OBC's are Making two third of Hindus. Now ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ngdharan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91</td>\n",
       "      <td>94</td>\n",
       "      <td>5323</td>\n",
       "      <td>2012-03-29 07:34:49</td>\n",
       "      <td>2020-07-21 02:56:27</td>\n",
       "      <td>123</td>\n",
       "      <td>Please Support &amp;amp; Maximum RT...\\n\\n#Brahman...</td>\n",
       "      <td>[{'text': 'BrahmanismRobsOBCReservation', 'ind...</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Nuetral</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EzhilJerry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tamil Nadu, India</td>\n",
       "      <td>133</td>\n",
       "      <td>88</td>\n",
       "      <td>11929</td>\n",
       "      <td>2011-07-10 03:39:34</td>\n",
       "      <td>2020-07-21 02:55:27</td>\n",
       "      <td>3</td>\n",
       "      <td>Of 89 secretaries in Modi govt, there are just...</td>\n",
       "      <td>[{'text': 'BrahmanismRobsOBCReservation', 'ind...</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username                                           acctdesc  \\\n",
       "0    PunniyakotiMu  Tamilian by Language. Indian by Land. Humane b...   \n",
       "1  Contractornesa5                                                NaN   \n",
       "2           vcrasu                                 \n",
       "3         ngdharan                                                NaN   \n",
       "4       EzhilJerry                                                NaN   \n",
       "\n",
       "            location  following  followers  totaltweets        usercreatedts  \\\n",
       "0     Chennai, India         67         14         3949  2010-12-15 04:01:28   \n",
       "1                NaN        425        132         2810  2019-05-31 03:23:24   \n",
       "2                NaN        158          5         1027  2020-06-04 08:45:04   \n",
       "3                NaN         91         94         5323  2012-03-29 07:34:49   \n",
       "4  Tamil Nadu, India        133         88        11929  2011-07-10 03:39:34   \n",
       "\n",
       "        tweetcreatedts  retweetcount  \\\n",
       "0  2020-07-21 03:01:18           711   \n",
       "1  2020-07-21 02:58:26            14   \n",
       "2  2020-07-21 02:58:20           344   \n",
       "3  2020-07-21 02:56:27           123   \n",
       "4  2020-07-21 02:55:27             3   \n",
       "\n",
       "                                                text  \\\n",
       "0  OBC population is 52%.\\nOBC Reservation is 27%...   \n",
       "1  @RangarajPandeyR In 1990, when VP Singh implem...   \n",
       "2  52% OBC's are Making two third of Hindus. Now ...   \n",
       "3  Please Support &amp; Maximum RT...\\n\\n#Brahman...   \n",
       "4  Of 89 secretaries in Modi govt, there are just...   \n",
       "\n",
       "                                            hashtags  TweetType  \\\n",
       "0                                                 []          1   \n",
       "1                                                 []          1   \n",
       "2                                                 []          1   \n",
       "3  [{'text': 'BrahmanismRobsOBCReservation', 'ind...          1   \n",
       "4  [{'text': 'BrahmanismRobsOBCReservation', 'ind...          1   \n",
       "\n",
       "  SVM_Prediction RF_Prediction LSTM_Prediction NaiveBayes_Prediction  \\\n",
       "0       Negative      Negative        Negative              Negative   \n",
       "1       Negative      Negative        Negative              Negative   \n",
       "2       Negative      Negative        Negative               Neutral   \n",
       "3       Negative       Nuetral        Negative              Negative   \n",
       "4       Negative      Negative        Negative              Negative   \n",
       "\n",
       "  Text_Blob_Prediction  \n",
       "0             Negative  \n",
       "1              Neutral  \n",
       "2             Positive  \n",
       "3              Neutral  \n",
       "4              Neutral  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 17)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "report1.to_csv(r'Report\\#BrahmanismRobsOBCReservationhashtagTweetsPrediction.csv', index=False)\n",
    "report2.to_csv(r'Report\\#ShameOnNews18hashtagTweetsPrediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_tweet = pd.read_csv('#ArrestKillersOfJayarajAndBennixhashtagTweets.csv', encoding = 'latin1')\n",
    "madeIndia_tweet = pd.read_csv('#MadeInIndiahashtagTweets.csv', encoding='latin1')\n",
    "modicare_tweet = pd.read_csv('#ModiCARES4PoorhashtagTweets.csv', encoding='latin1')\n",
    "stop_tweet = pd.read_csv('#StopBhaashanTakeActionhashtagTweets.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_tweet = arrest_tweet[['text','tweetcreatedts']]\n",
    "# madeIndia_tweet = madeIndia_tweet[['text','tweetcreatedts']]\n",
    "# modicare_tweet = modicare_tweet[['text', 'tweetcreatedts']]\n",
    "# stop_tweet = stop_tweet[['text', 'tweetcreatedts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = arrest_tweet\n",
    "final2 = madeIndia_tweet\n",
    "final3 = modicare_tweet\n",
    "final4 = stop_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_tweet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnseenData cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataclean(tweet):\n",
    "    tweet['text'] = tweet['text'].apply(lambda x: x.lower())\n",
    "    tweet['text'] = tweet['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    for idx,row in tweet.iterrows():\n",
    "        row[0] = row[0].replace('rt',' ')     \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = get_dataclean(arrest_tweet)\n",
    "data2 = get_dataclean(madeIndia_tweet)\n",
    "data3 = get_dataclean(modicare_tweet)\n",
    "data4 = get_dataclean(stop_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of Unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(nb_words=max_fatures, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function for data cleaning\n",
    "def Get_tokens(data):\n",
    "    tokenizer.fit_on_texts(data['text'].values)\n",
    "    g_token = tokenizer.texts_to_sequences(data['text'].values)\n",
    "    g_pad = pad_sequences(g_token, maxlen=100)\n",
    "    return g_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token1 = Get_tokens(data1)\n",
    "token2 = Get_tokens(data2)\n",
    "token3 = Get_tokens(data3)\n",
    "token4 = Get_tokens(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token1),  token1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_unseenPred = SVC.predict(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(sentiment_score):\n",
    "    senti = []\n",
    "    for i in sentiment_score:\n",
    "        if i == 1:\n",
    "            senti.append(\"Positive\")\n",
    "        elif i == 0:\n",
    "            senti.append(\"Negative\")\n",
    "        else:\n",
    "            senti.append(\"Nuetral\")\n",
    "    return senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Model(token,model):\n",
    "    prds = model.predict(token)\n",
    "    return prds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_pred_svm = only_model(token1, SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = Get_Model(token1, SVC)\n",
    "df2 = Get_Model(token2, SVC)\n",
    "df3 = Get_Model(token3, SVC)\n",
    "df4 = Get_Model(token4, SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_pred_svm = get_type(series_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_type(df1)\n",
    "df2 = get_type(df2)\n",
    "df3 = get_type(df3)\n",
    "df4 = get_type(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Pred_testData(token,model, colName):\n",
    "#     prds = model.predict(token)\n",
    "#     df = pd.DataFrame(prds)\n",
    "#     df_re = df.rename(columns={0: colName})\n",
    "#     return df_re\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Pred_testData(token,model, colName):\n",
    "#     prds = model.predict(token)\n",
    "#     df = pd.DataFrame(prds)\n",
    "#     df_re = df.rename(columns={0: colName})\n",
    "#     return df_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_pred_RF = only_model(token1, Classifier_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_RFScore = Get_Model(token1, Classifier_RF)\n",
    "df2_RFScore = Get_Model(token2, Classifier_RF)\n",
    "df3_RFScore = Get_Model(token3, Classifier_RF)\n",
    "df4_RFScore = Get_Model(token4, Classifier_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_RF = get_type(df1_RFScore)\n",
    "df2_RF = get_type(df2_RFScore)\n",
    "df3_RF = get_type(df3_RFScore)\n",
    "df4_RF = get_type(df4_RFScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1_RF = Pred_testData(token1,Classifier_RF, colName='RF_Prediction')\n",
    "# df2_RF = Pred_testData(token2, Classifier_RF, colName='RF_Prediction')\n",
    "# df3_RF = Pred_testData(token3, Classifier_RF, colName='RF_Prediction')\n",
    "# df4_RF = Pred_testData(token4, Classifier_RF, colName='RF_Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1_RF.RF_Prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1_RF['RF_Prediction'] = get_SetimentType(df1_RF['RF_Prediction'])\n",
    "# df2_RF['RF_Prediction'] = get_SetimentType(df2_RF['RF_Prediction'])\n",
    "# df3_RF['RF_Prediction'] = get_SetimentType(df3_RF['RF_Prediction'])\n",
    "# df4_RF['RF_Prediction'] = get_SetimentType(df4_RF['RF_Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1_RF.RF_Prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Get_LSTM(token,model):\n",
    "#     prds = model.predict(token)\n",
    "#     score = [np.argmax(i) for i in prds]\n",
    "#     return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_lstmScore = Get_LSTM(token1, model)\n",
    "df2_lstmScore = Get_LSTM(token2, model)\n",
    "df3_lstmScore = Get_LSTM(token3, model)\n",
    "df4_lstmScore = Get_LSTM(token4, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_lstm = get_type(df1_lstmScore)\n",
    "df2_lstm = get_type(df2_lstmScore)\n",
    "df3_lstm = get_type(df3_lstmScore)\n",
    "df4_lstm = get_type(df4_lstmScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_NB_pred(data,model):\n",
    "    res = []\n",
    "    for obj in data.text:\n",
    "        res.append(model.classify(extract_features(obj.split())))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = Get_NB_pred(arrest_tweet, classifier)\n",
    "result2 = Get_NB_pred(madeIndia_tweet, classifier)\n",
    "result3 = Get_NB_pred(modicare_tweet, classifier)\n",
    "result4 = Get_NB_pred(stop_tweet, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = arrest_tweet\n",
    "final2 = madeIndia_tweet\n",
    "final3 = modicare_tweet\n",
    "final4 = stop_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['polarity'] = data1['text'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "data2['polarity'] = data2['text'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "data3['polarity'] = data3['text'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "data4['polarity'] = data4['text'].map(lambda text: TextBlob(text).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function for converting sentiment type as Positive if sentiment score is (>0), neutral if sentiment score is (=0), \n",
    "#negative if sentiment score is (<0)\n",
    "\n",
    "def Get_Polarity(_polarity):\n",
    "    sentiment = \"Positive\"\n",
    "    if _polarity > 0:\n",
    "        sentiment= 'Positive'\n",
    "                \n",
    "    elif _polarity == 0:\n",
    "        sentiment = 'Neutral'\n",
    "            \n",
    "    else:\n",
    "        sentiment = 'Negative'\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity1 = data1.polarity.apply(Get_Polarity)\n",
    "polarity2 = data2.polarity.apply(Get_Polarity)\n",
    "polarity3 = data3.polarity.apply(Get_Polarity)\n",
    "polarity4 = data4.polarity.apply(Get_Polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinalReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_report(data, svmSer, rfSer, lstmSer, NBPreds, PolarityPreds):\n",
    "    report = pd.DataFrame()\n",
    "    report = data\n",
    "    report['SVM_Prediction'] = svmSer\n",
    "    report['RF_Prediction'] = rfSer\n",
    "    report['LSTM_Prediction'] = lstmSer\n",
    "    report['NaiveBayes_Prediction'] = NBPreds\n",
    "    report['Text_Blob_Prediction'] = PolarityPreds\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report1 = Final_report(final1, df1, df1_RF,df1_lstm, result1, polarity1)\n",
    "report2 = Final_report(final2, df2, df2_RF, df2_lstm, result2, polarity2)\n",
    "report3 = Final_report(final3, df3, df3_RF, df3_lstm, result3, polarity3)\n",
    "report4 = Final_report(final4, df4, df4_RF, df4_lstm, result4, polarity4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report1.to_csv(r'Report\\#ArrestKillersOfJayarajAndBennixhashtagTweets_Report.csv', index=False)\n",
    "report2.to_csv(r'Report\\#MadeInIndiahashtagTweets_Report.csv', index=False)\n",
    "report3.to_csv(r'Report\\#ModiCARES4PoorhashtagTweets_Report.csv', index=False)\n",
    "report4.to_csv(r'Report\\#StopBhaashanTakeActionhashtagTweets_Report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 185,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
